{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR8IrxkEaGjw",
        "outputId": "be6b0a59-9520-4174-be56-d756397c9a64"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8OX0yGBhdXQ",
        "outputId": "0b975ba5-2912-4576-e82b-a42401f2a802"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_project'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs285_project'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvFWD33SWts8"
      },
      "outputs": [],
      "source": [
        "%cd $SYM_PATH\n",
        "# !git clone https://github.com/MyEncyclopedia/ConnectNGym.git\n",
        "# !ls\n",
        "# %cd ConnectNGym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVvJy97yWyVs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "logdir = 'gomoku_model' + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "print(\"LOGGING TO: \", logdir)\n",
        "%cd $logdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BNB5hebhgVy",
        "outputId": "40a3b752-047e-4dca-fc0f-0996d4e73d16"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Python CNN+DQN 5x5 get 4 Gomoku - Mark Kang Oct.2019\n",
        "'''\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import keras\n",
        "import sys\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers, metrics\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "EPISODES = 4000\n",
        "\n",
        "EMPTY_SPACE = 0\n",
        "WHITE_PLAYER = -1\n",
        "BLACK_PLAYER = 1\n",
        "\n",
        "# define width and length of MYMAP, it will be used in this way:\n",
        "# MYMAP[Y_OF_MYMAP][X_OF_MYMAP]\n",
        "Y_OF_MYMAP = 7\n",
        "X_OF_MYMAP = 7\n",
        "# How many continues chess is WIN?\n",
        "WIN_CONDITION = 4\n",
        "\n",
        "MYMAP = np.zeros((X_OF_MYMAP,Y_OF_MYMAP), dtype=int)\n",
        "\n",
        "Running_MAP = 0 # runtime map, it will initialized as MYMAP.\n",
        "\n",
        "# Define total actions\n",
        "gTOTAL_ACTIONs = Y_OF_MYMAP * X_OF_MYMAP\n",
        "\n",
        "# NN parameters\n",
        "gGAMMA = 0.75\n",
        "gLEARNNING_RATE = 0.00001\n",
        "\n",
        "# Other parameters\n",
        "WINNER_REWARD = 1\n",
        "LOSER_REWARD = -1\n",
        "DRAW_REWARD = 0\n",
        "\n",
        "#############################\n",
        "\n",
        "def env_render(pMYMAP):\n",
        "    for y in range(0,Y_OF_MYMAP,1):\n",
        "        for x in range(0,X_OF_MYMAP,1):\n",
        "            if pMYMAP[0][x+y*Y_OF_MYMAP] == WHITE_PLAYER:\n",
        "                print(\"O\",end=\"\")\n",
        "            elif pMYMAP[0][x+y*Y_OF_MYMAP]  == BLACK_PLAYER:\n",
        "                print(\"X\",end=\"\")\n",
        "            elif pMYMAP[0][x+y*Y_OF_MYMAP]  == EMPTY_SPACE:\n",
        "                print(\"_\",end=\"\")\n",
        "            else:\n",
        "                print(\"PANIC, unknow element of MYMAP\")\n",
        "                exit()\n",
        "        print(\"\") # chnage to new line\n",
        "\n",
        "def check_state_has_winner(nn_state, who_is_playing):\n",
        "    # Check every X line\n",
        "    for y in range(0, gSTATE_SIZE, Y_OF_MYMAP):\n",
        "        for x in range(0, X_OF_MYMAP-WIN_CONDITION+1,1):\n",
        "            chess_count = 0\n",
        "            if y+x+WIN_CONDITION-1 < y+X_OF_MYMAP:\n",
        "                for j in range(0, WIN_CONDITION, 1):\n",
        "                    if nn_state[0][y+x+j] == who_is_playing:                \n",
        "                        chess_count=chess_count+1\n",
        "                        if chess_count == WIN_CONDITION:\n",
        "                            return True\n",
        "                    else:\n",
        "                        break\n",
        "                    \n",
        "    # Check every Y line\n",
        "    for x in range(0, X_OF_MYMAP, 1):\n",
        "        for y in range(0, Y_OF_MYMAP-WIN_CONDITION+1,1):\n",
        "            chess_count = 0\n",
        "            if x+X_OF_MYMAP*y+X_OF_MYMAP*(WIN_CONDITION-1) < gSTATE_SIZE:\n",
        "                for j in range(0, WIN_CONDITION, 1):\n",
        "                    if nn_state[0][x+X_OF_MYMAP*y+X_OF_MYMAP*j] == who_is_playing:\n",
        "                        chess_count=chess_count+1\n",
        "                        if chess_count == WIN_CONDITION:\n",
        "                            return True                \n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "    # Check \\\n",
        "    for y in range(0, Y_OF_MYMAP, 1):\n",
        "        for x in range(0, X_OF_MYMAP, 1):\n",
        "            chess_count = 0\n",
        "            for j in range(0,WIN_CONDITION,1):\n",
        "                if (y+WIN_CONDITION-1)<Y_OF_MYMAP and (x+WIN_CONDITION-1)<X_OF_MYMAP:\n",
        "                    if nn_state[0][x+X_OF_MYMAP*y+X_OF_MYMAP*j+j] == who_is_playing:\n",
        "                        chess_count=chess_count+1\n",
        "                        if chess_count == WIN_CONDITION:\n",
        "                            return True\n",
        "                    else:\n",
        "                        break\n",
        "                        \n",
        "    # check / \n",
        "    for y in range(0, Y_OF_MYMAP, 1):\n",
        "        for x in range(X_OF_MYMAP - 1, -1, -1):\n",
        "            chess_count = 0\n",
        "            for j in range(0,WIN_CONDITION,1):\n",
        "                if (y+WIN_CONDITION-1)<Y_OF_MYMAP and (x-(WIN_CONDITION-1)) >= 0:\n",
        "                    if nn_state[0][x+X_OF_MYMAP*y+X_OF_MYMAP*j-j] == who_is_playing:\n",
        "                        chess_count=chess_count+1\n",
        "                        if chess_count == WIN_CONDITION:\n",
        "                            return True\n",
        "                    else:\n",
        "                        break\n",
        "                        \n",
        "    # No winner\n",
        "    return False\n",
        "\n",
        "def env_step(nn_state, internal_action,who_is_playing):\n",
        "    done = False\n",
        "    reward = 0\n",
        "    private_next_state = copy.deepcopy(nn_state)\n",
        "\n",
        "    if private_next_state[0][internal_action] != EMPTY_SPACE:\n",
        "        print (\"PANIC, env_step(),  private_next_state[internal_action] != EMPTY_SPACE\")\n",
        "    else:\n",
        "        private_next_state[0][internal_action] = who_is_playing\n",
        "\n",
        "    done = check_state_has_winner(private_next_state,who_is_playing)\n",
        "    if done == True:\n",
        "        reward = 100\n",
        "        return private_next_state, reward, done, 0\n",
        "\n",
        "    for index in range (0, Y_OF_MYMAP * X_OF_MYMAP ,1):\n",
        "        if private_next_state[0][index] != EMPTY_SPACE:\n",
        "            continue\n",
        "        else:\n",
        "            return private_next_state, reward, done, 0\n",
        "\n",
        "    done = True # Full of map, but no winner\n",
        "    return private_next_state, reward, done, 0\n",
        "\n",
        "####################################################\n",
        "# 2019/09/27\n",
        "# In order to use CNN, convert 1D state to 2D array.\n",
        "####################################################\n",
        "def convert_1D_state_to_2D_array(_state_1D):\n",
        "    _state_2d_array = np.zeros((1,X_OF_MYMAP,Y_OF_MYMAP,1), dtype=int) # Keras CNN needs 4-D array as input.\n",
        "    for y in range(0,Y_OF_MYMAP):\n",
        "        for x in range(0, X_OF_MYMAP):\n",
        "            _state_2d_array[0][y][x][0] = _state_1D[0][ y*Y_OF_MYMAP + x]\n",
        "\n",
        "    return _state_2d_array\n",
        "\n",
        "\n",
        "class CQLAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size =  state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=6000)\n",
        "        self.gamma = gGAMMA    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.3\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning_rate = gLEARNNING_RATE\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_freq=3000\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        # model.add(Dense(512, input_dim=self.state_size, activation='linear'))\n",
        "        ##################################################\n",
        "        model.add(Conv2D(1024, kernel_size=(3,3), activation='linear', input_shape=(Y_OF_MYMAP, X_OF_MYMAP, 1)))\n",
        "        model.add(Conv2D(1024, (2, 2), activation='linear'))   \n",
        "        model.add(Conv2D(1024, (2, 2), activation='linear'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='linear'))\n",
        "        model.add(Dense(512, activation='relu'))        \n",
        "        model.add(Dense(512, activation='linear'))         \n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        #model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta())\n",
        "        #model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        #model.compile(loss='mse',optimizer=optimizers.SGD(lr=0.0001, clipnorm=1.))\n",
        "        #model.compile(loss='mse',optimizer=keras.optimizers.Adagrad(lr=gLEARNNING_RATE, epsilon=None, decay=0.0))\n",
        "        model.compile(loss='mse',optimizer=keras.optimizers.RMSprop(lr=gLEARNNING_RATE,rho=0.9, epsilon=None, decay=0.0), metrics=[metrics.MeanSquaredError()])\n",
        "        #model.compile(loss='mse', optimizer=keras.optimizers.Adadelta())\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def predict(self, nn_state):\n",
        "        return self.model.predict(nn_state)\n",
        "\n",
        "    def act(self, nn_state, who_is_playing, _time, test=False):\n",
        "        internal_action = -1\n",
        "        available_location = copy.deepcopy(MYMAP)\n",
        "        available_location = np.reshape(available_location, [1, gSTATE_SIZE])\n",
        "        available_location_count = 0\n",
        "\n",
        "        state_2d=convert_1D_state_to_2D_array(nn_state)\n",
        "\n",
        "        for x in range (0, Y_OF_MYMAP * X_OF_MYMAP ,1):\n",
        "            if (nn_state[0][x] == EMPTY_SPACE ):\n",
        "                available_location[0][available_location_count] = x\n",
        "                available_location_count=available_location_count+1\n",
        "\n",
        "        while (True):\n",
        "            #if np.random.rand() <= self.epsilon or who_is_playing == WHITE_PLAYER:\n",
        "            #if np.random.rand() <= self.epsilon or who_is_playing == BLACK_PLAYER:\n",
        "            if np.random.rand() <= self.epsilon:\n",
        "                index = random.randrange(0, available_location_count,1)\n",
        "                internal_action = available_location[0][index]\n",
        "                if nn_state[0][internal_action] != EMPTY_SPACE:\n",
        "                    internal_action = -1\n",
        "                    print(\"random choosing new action\")\n",
        "            else:\n",
        "                act_values = self.model.predict(state_2d)\n",
        "                internal_action = np.argmax(act_values[0])\n",
        "                #print(\"AI\")\n",
        "                if nn_state[0][internal_action] != EMPTY_SPACE:\n",
        "                    #print(\" AI missed\")\n",
        "                    max_q = -99999\n",
        "                    for p in range (0,available_location_count,1 ):\n",
        "                        if max_q < act_values[0][available_location[0][p]]:\n",
        "                            max_q = act_values[0][available_location[0][p]]\n",
        "                            internal_action = available_location[0][p]\n",
        "                        if nn_state[0][internal_action] != EMPTY_SPACE:\n",
        "                            internal_action = -1\n",
        "            \n",
        "            if test:\n",
        "              if internal_action > -1:\n",
        "                break\n",
        "            else:\n",
        "              ####################################################################################\n",
        "              # One step win check and compare AI predict result to see training is converged yet.\n",
        "              ####################################################################################\n",
        "              for _index in range (0, available_location_count,1):\n",
        "                  _osc_Action = available_location[0][_index]\n",
        "                  _osc_next_state, _osc_reward, _osc_done, _ = env_step(nn_state, _osc_Action, Who_is_playing)\n",
        "                  if _osc_reward > 0:\n",
        "                      ######\n",
        "                      if internal_action==_osc_Action:\n",
        "                          print(\"Who_is_playing = {}, AI action hit OneStepWinCheck action, action={}\".format(Who_is_playing,_osc_Action))\n",
        "                      else:\n",
        "                          print(\"Who_is_playing = {}, OneStepWinCheck overrides AI predicted data\".format(Who_is_playing))\n",
        "                      internal_action = _osc_Action\n",
        "                      return internal_action, self.model.predict(state_2d)\n",
        "\n",
        "              ####################################################################################\n",
        "              # One-step-lose check and compare AI predict result to see training is converged yet.\n",
        "              ####################################################################################\n",
        "              for _index in range (0, available_location_count,1):\n",
        "                  _osc_Action = available_location[0][_index]\n",
        "                  _osc_next_state, _osc_reward, _osc_done, _ = env_step(nn_state, _osc_Action, -1 * Who_is_playing)\n",
        "                  if _osc_reward > 0:\n",
        "                      ######\n",
        "                      if internal_action==_osc_Action:\n",
        "                          print(\"Who_is_playing = {}, AI action hit OneStepLoseCheck action, action = {}\".format(Who_is_playing,_osc_Action))\n",
        "                      else:\n",
        "                          print(\"Who_is_playing = {}, OneStepLoseCheck overrides AI predicted data\".format(Who_is_playing))\n",
        "                      internal_action = _osc_Action\n",
        "                      return internal_action, self.model.predict(state_2d)\n",
        "\n",
        "              if internal_action > -1:\n",
        "                  break\n",
        "\n",
        "        return internal_action, self.model.predict(state_2d)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        ######### Orignal fetch \n",
        "        #minibatch = random.sample(self.memory, batch_size)\n",
        "        #########\n",
        "        print(\"replayingg\")\n",
        "        minibatch = []\n",
        "        _memory_length = len(self.memory)\n",
        "        _state_all=np.zeros(( _memory_length,gSTATE_SIZE), dtype=float)\n",
        "        _CNN_all_state=np.zeros(( _memory_length, Y_OF_MYMAP, X_OF_MYMAP,1), dtype=float)\n",
        "        _target_f_all=np.zeros(( _memory_length,gSTATE_SIZE), dtype=float)\n",
        "        _index = 0\n",
        "\n",
        "        for _ in range(_memory_length):            \n",
        "            #minibatch.append(self.memory.popleft()) \n",
        "            minibatch.append(self.memory.pop()) \n",
        "\n",
        "        for state, action, replay_reward, next_state, done in minibatch:\n",
        "            CNN_signle_state=convert_1D_state_to_2D_array(state)\n",
        "            ###############\n",
        "\n",
        "            internal_action = -1\n",
        "            unavailable_location = copy.deepcopy(MYMAP)\n",
        "            unavailable_location = np.reshape(unavailable_location, [1, gSTATE_SIZE])\n",
        "            unavailable_location_count = 0           \n",
        "            for x in range (0, Y_OF_MYMAP * X_OF_MYMAP ,1):\n",
        "                if (state[0][x] != EMPTY_SPACE ):\n",
        "                    unavailable_location[0][unavailable_location_count] = x\n",
        "                    unavailable_location_count=unavailable_location_count+1\n",
        "            #################\n",
        "            re_n=(replay_reward+1)*100\n",
        "            CNN_signle_state_next=convert_1D_state_to_2D_array(next_state)\n",
        "            qa_t_values = self.model.predict(CNN_signle_state)\n",
        "            q_t_values=tf.convert_to_tensor([qa_t_values[0][action]])\n",
        "            # if not done:\n",
        "            qa_tp1_values = self.target_model(CNN_signle_state_next)\n",
        "            next_actions=np.argmax(self.model.predict(CNN_signle_state_next))\n",
        "            q_tp1=tf.convert_to_tensor([qa_tp1_values[0][next_actions]])\n",
        "            target = tf.stop_gradient(re_n + self.gamma * q_tp1)\n",
        "            loss =tf.math.reduce_mean((q_t_values-target)**2)    \n",
        "            q_t_logsumexp =  tf.math.reduce_logsumexp(qa_t_values,1) \n",
        "            cql_loss=tf.math.reduce_mean(q_t_logsumexp - q_t_values)\n",
        "            loss=loss+0.05*cql_loss\n",
        "            target_f = self.model.predict(CNN_signle_state)\n",
        "            target_f[0][action] = loss\n",
        "        \n",
        "            ####\n",
        "            for x in range (0, unavailable_location_count, 1):\n",
        "                target_f[0][unavailable_location[0][x]] = LOSER_REWARD * 2\n",
        "            ####\n",
        "            if _index == 0:\n",
        "                _CNN_all_state = copy.deepcopy(CNN_signle_state)\n",
        "            else:\n",
        "                _CNN_all_state = np.vstack((_CNN_all_state,CNN_signle_state))\n",
        "                \n",
        "            _target_f_all[_index]=copy.deepcopy(target_f)\n",
        "            \n",
        "            _index = _index + 1                    \n",
        "        csv_logger = CSVLogger('log_cql_new.csv', append=True, separator=';')         \n",
        "        self.model.fit(_CNN_all_state, _target_f_all, batch_size=128, shuffle=False, epochs=1, verbose=2, callbacks=[csv_logger])\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_network(self):\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "def play_game(agent_one, agent_two):\n",
        "  done = False \n",
        "  player = 1\n",
        "  time = 0\n",
        "  x_len = 7 # game board row\n",
        "  y_len = 7 # game board col\n",
        "  map = np.zeros((x_len, y_len), dtype=int) # current map\n",
        "  NN_state = np.reshape(map, [1, x_len*y_len])\n",
        "\n",
        "  while not done:\n",
        "    if player == 1:\n",
        "      action, _ = agent_one.act(NN_state, 1, time, test=True)\n",
        "    else:\n",
        "      action, _ = agent_two.act(NN_state, -1, time, test=True)\n",
        "\n",
        "    NN_state, reward, done, _ = env_step(NN_state, action, player)\n",
        "    # print(NN_state)\n",
        "    \n",
        "    if done and reward == 0:\n",
        "      env_render(NN_state)\n",
        "      print(\"done for the testing game\", 0)\n",
        "      return 0\n",
        "    elif done:\n",
        "      env_render(NN_state)\n",
        "      print(\"done for the testing game\", player)\n",
        "      return player\n",
        "    player = -player\n",
        "  return '?'\n",
        "\n",
        "def run_tournament(agents, n_iter=1000):\n",
        "  # Shuffle the agents\n",
        "  agent_weights = np.array([1] * len(agents))\n",
        "\n",
        "  # run the tournament once, knocking out each team after they lose. The last team standing is the winner\n",
        "  for _ in range(n_iter):\n",
        "      # randomly select two agents to play\n",
        "      idx1, idx2 = np.random.choice(agents, 2, replace=False, p=agent_weights / sum(agent_weights))\n",
        "      agent1 = agents[idx1]\n",
        "      agent2 = agents[idx2]\n",
        "\n",
        "      # play the game\n",
        "      winner_id, _ = play_game(agent1, agent2)\n",
        "      # remove the loser from the tournament\n",
        "      if winner_id == 1:\n",
        "        agent_weights[idx2] *= .5\n",
        "      elif winner_id == -1:\n",
        "        agent_weights[idx1] *= .5\n",
        "      else:\n",
        "        # Assymetric reward for tie bc player 1 has an advantage\n",
        "        agent_weights[idx1] *= .8\n",
        "        agent_weights[idx2] *= .9\n",
        "\n",
        "  return agent_weights\n",
        "\n",
        "def get_best_agent(agents):\n",
        "  '''\n",
        "  Get the best agent from a list of agents. \n",
        "  '''\n",
        "  agent_weights = run_tournament(agents)\n",
        "  best_agent = agents[np.argmax(agent_weights)]\n",
        "  return best_agent\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gSTATE_SIZE = X_OF_MYMAP * Y_OF_MYMAP\n",
        "    Agent_Black = CQLAgent(gSTATE_SIZE, gTOTAL_ACTIONs)\n",
        "    Agent_White = CQLAgent(gSTATE_SIZE, gTOTAL_ACTIONs)\n",
        "    Agent_test = CQLAgent(gSTATE_SIZE, gTOTAL_ACTIONs)\n",
        "\n",
        "    Win_Black_Count = 0\n",
        "    Win_White_Count = 0\n",
        "    Win_Draw_Count = 0\n",
        "\n",
        "    done = False\n",
        "    BLACK_batch_size = 96\n",
        "    WHITE_batch_size = 96\n",
        "\n",
        "    win_rate_first = []\n",
        "    tie_rate_first = []\n",
        "    win_rate_second = []\n",
        "    tie_rate_second = []\n",
        "    time_step=0\n",
        "    prev_step=0\n",
        "    for e in range(EPISODES+1):\n",
        "        # Enviroment Reset\n",
        "        Running_MAP = copy.deepcopy(MYMAP)\n",
        "        Who_is_playing  = BLACK_PLAYER\n",
        "        BLACK_Last_Action = -1\n",
        "        WHITE_Last_Action = -1\n",
        "        \n",
        "        NN_state = np.reshape(Running_MAP, [1, gSTATE_SIZE]) # reshape Running_MAP to NN input form\n",
        "        BLACK_Last_State = copy.deepcopy(NN_state)\n",
        "        \n",
        "        #### 20191015 his feature, history feature.\n",
        "        BLACK_History_State = np.zeros((X_OF_MYMAP*Y_OF_MYMAP + 1, 1, X_OF_MYMAP * Y_OF_MYMAP), dtype=int)                        \n",
        "        WHITE_History_State = np.zeros((X_OF_MYMAP*Y_OF_MYMAP + 1, 1, X_OF_MYMAP * Y_OF_MYMAP), dtype=int)        \n",
        "\n",
        "        BLACK_History_Action = np.zeros((X_OF_MYMAP*Y_OF_MYMAP + 1), dtype=int)\n",
        "        WHITE_History_Action = np.zeros((X_OF_MYMAP*Y_OF_MYMAP + 1), dtype=int)\n",
        "        #### 20191015 his feature, history feature.\n",
        "\n",
        "        #env_render(NN_state)\n",
        "        \n",
        "        for time in range(1, Y_OF_MYMAP * X_OF_MYMAP+1, 1):\n",
        "            if (Who_is_playing == WHITE_PLAYER):\n",
        "                Action,Q_Values = Agent_White.act(NN_state, Who_is_playing, time)\n",
        "\n",
        "            elif (Who_is_playing == BLACK_PLAYER):\n",
        "                Action,Q_Values = Agent_Black.act(NN_state, Who_is_playing, time)\n",
        "\n",
        "            ####### Hard coding actions for debug START ######### \n",
        "            if time == 1 and Who_is_playing == BLACK_PLAYER:               \n",
        "                Action = 12\n",
        "            VERIFY_ROUND = 15\n",
        "            #\n",
        "            if time == VERIFY_ROUND:\n",
        "               env_render(NN_state)\n",
        "\n",
        "            ###################\n",
        "            next_state, reward, done, _ = env_step(NN_state, Action, Who_is_playing) #env.step(action)\n",
        "            ###################\n",
        "            if Who_is_playing == BLACK_PLAYER:\n",
        "                #### 20191015 his feature, history feature.\n",
        "                BLACK_History_State[time] = copy.deepcopy(NN_state)\n",
        "                BLACK_History_Action[time] = Action\n",
        "                #### 20191015 his feature, history feature.\n",
        "\n",
        "            if Who_is_playing == WHITE_PLAYER:\n",
        "                #### 20191015 his feature, history feature.\n",
        "                WHITE_History_State[time] = copy.deepcopy(NN_state)\n",
        "                WHITE_History_Action[time] = Action\n",
        "                #### 20191015 his feature, history feature.\n",
        "\n",
        "            if reward == 0:      # DRAW\n",
        "                BLACK_reward = 0 # TODO may not hard code here.\n",
        "                WHITE_reward = 0 # TODO may not hard code here.\n",
        "\n",
        "            if done and reward == 100: # Someone WIN\n",
        "                if Who_is_playing == BLACK_PLAYER:      # BLACK_PLAYER WIN\n",
        "                    BLACK_reward = WINNER_REWARD\n",
        "                    WHITE_reward = LOSER_REWARD\n",
        "                 \n",
        "                    Agent_Black.remember(NN_state, Action, WINNER_REWARD, next_state, True)\n",
        "                    _count = 1\n",
        "                    for _end_index in range (time, 1, -2 ):                        \n",
        "                        Agent_Black.remember(BLACK_History_State[_end_index-2], BLACK_History_Action[_end_index-2], WINNER_REWARD * ( gGAMMA ** _count) , BLACK_History_State[_end_index], True)\n",
        "                        _count = _count + 1\n",
        "                    _count = 1                    \n",
        "                    for _end_index in range (time-1,1,-2):\n",
        "                        Agent_White.remember(WHITE_History_State[_end_index-2], WHITE_History_Action[_end_index-2], LOSER_REWARD  * ( gGAMMA ** _count), WHITE_History_State[_end_index], True)\n",
        "                        _count = _count + 1\n",
        "\n",
        "                elif Who_is_playing == WHITE_PLAYER:    # WHITE_PLAYER WIN\n",
        "                    BLACK_reward = LOSER_REWARD\n",
        "                    WHITE_reward = WINNER_REWARD\n",
        "     \n",
        "                    Agent_White.remember(NN_state, Action, WINNER_REWARD, next_state, True)\n",
        "                    _count = 1\n",
        "                    for _end_index in range (time, 1, -2 ):                        \n",
        "                        Agent_White.remember(WHITE_History_State[_end_index-2], WHITE_History_Action[_end_index-2], WINNER_REWARD * ( gGAMMA ** _count) , WHITE_History_State[_end_index], True)\n",
        "                        _count = _count + 1\n",
        "                    _count = 1                    \n",
        "                    for _end_index in range (time-1,1,-2):\n",
        "                        Agent_Black.remember(BLACK_History_State[_end_index-2], BLACK_History_Action[_end_index-2], LOSER_REWARD  * ( gGAMMA ** _count), BLACK_History_State[_end_index], True)\n",
        "                        _count = _count + 1\n",
        "\n",
        "                else:\n",
        "                    print (\"PANIC - unknown who is Winner\")\n",
        "                    sys.exit()\n",
        "                     \n",
        "            elif done and reward ==0: ## DRAW\n",
        "                if Who_is_playing == BLACK_PLAYER:\n",
        "                    Agent_Black.remember(NN_state, Action, DRAW_REWARD, next_state, True)\n",
        "\n",
        "                if Who_is_playing == WHITE_PLAYER:\n",
        "                    Agent_White.remember(NN_state, Action, DRAW_REWARD, next_state, True)\n",
        "\n",
        "                _count = 1\n",
        "                for _end_index in range (time, 1, -2 ):                        \n",
        "                    Agent_Black.remember(BLACK_History_State[_end_index-2], BLACK_History_Action[_end_index-2], DRAW_REWARD, BLACK_History_State[_end_index], True)\n",
        "                    _count = _count + 1\n",
        "                _count = 1                    \n",
        "                for _end_index in range (time-1,1,-2):\n",
        "                    Agent_White.remember(WHITE_History_State[_end_index-2], WHITE_History_Action[_end_index-2], DRAW_REWARD, WHITE_History_State[_end_index], True)\n",
        "                    _count = _count + 1                       \n",
        "            NN_state = copy.deepcopy(next_state) # update NN_state to new state\n",
        "\n",
        "            # if len(Agent_Black.memory) > BLACK_batch_size and done == True:\n",
        "            if len(Agent_Black.memory) > BLACK_batch_size and done == True:\n",
        "                Agent_Black.replay(BLACK_batch_size)\n",
        "                time_step+=1\n",
        "            if len(Agent_White.memory) > WHITE_batch_size and done == True:\n",
        "                Agent_White.replay(WHITE_batch_size)\n",
        "                time_step+=1\n",
        "            print(\"step\", time_step)\n",
        "            \n",
        "            if time_step%10==0 and time_step!=prev_step:\n",
        "              prev_step=time_step\n",
        "              Agent_Black.update_target_network()\n",
        "              Agent_White.update_target_network()\n",
        "\n",
        "            if done == True:   # This round is finished.\n",
        "\n",
        "                if reward>0:\n",
        "                    if Who_is_playing == BLACK_PLAYER:\n",
        "                        Win_Black_Count = Win_Black_Count+1\n",
        "                    elif Who_is_playing == WHITE_PLAYER:\n",
        "                        Win_White_Count = Win_White_Count+1\n",
        "                    else:\n",
        "                        print (\"PANIC, unknow know who is winner\")\n",
        "                        \n",
        "                elif reward == 0:\n",
        "                    Win_Draw_Count = Win_Draw_Count + 1\n",
        "                                \n",
        "                if (Win_Black_Count+Win_White_Count)>0:\n",
        "                    print(\"Action={} BlackWinRate={} Black Wins={} White Wins={} Draw={} episode: {}/{}, used_step: {}, epsilon: {:.2}\"\n",
        "                           .format(Action, Win_Black_Count/(Win_Black_Count+Win_White_Count), Win_Black_Count,Win_White_Count, Win_Draw_Count, e, EPISODES, time, Agent_Black.epsilon))\n",
        "                    if e%200==0:\n",
        "                        Agent_Black.save(\"7x7_get_4_osc_33cnn_his_BLACK.h5\")\n",
        "                        Agent_White.save(\"7x7_get_4_osc_33cnn_his_WHITE.h5\")   \n",
        "                    if e%200==0:\n",
        "                      test = 40\n",
        "                      win = 0\n",
        "                      draw =0\n",
        "\n",
        "                      temp_b = Agent_Black.epsilon\n",
        "                      Agent_Black.epsilon = 0.0\n",
        "\n",
        "                      for i in range(test):\n",
        "                        result = play_game(Agent_Black, Agent_test)\n",
        "                        if result == 1:\n",
        "                          win +=1\n",
        "                        elif result == 0:\n",
        "                          draw +=1\n",
        "                      \n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going first)is: \", win/test, \"draw rate is: \", draw/test)\n",
        "\n",
        "                      win_rate_first.append(win/test)\n",
        "                      tie_rate_first.append(draw/test)\n",
        "\n",
        "                      win_sec = 0\n",
        "                      draw_sec = 0\n",
        "                      for i in range(test):\n",
        "                        result = play_game(Agent_test, Agent_Black)\n",
        "                        if result == -1:\n",
        "                          win_sec +=1\n",
        "                        elif result == 0:\n",
        "                          draw_sec +=1\n",
        "\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      print('after ', e, ' iterations', \"the win rate against random (going second)is: \", win_sec/test, \"draw rate is: \", draw_sec/test)\n",
        "                      \n",
        "                      win_rate_second.append(win_sec/test)\n",
        "                      tie_rate_second.append(draw_sec/test)\n",
        "\n",
        "                      Agent_Black.epsilon = temp_b # change it back to training stage\n",
        "\n",
        "\n",
        "                break\n",
        "\n",
        "            Who_is_playing = -1 * Who_is_playing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "rxNlYxRpzxiu",
        "outputId": "9e79902e-8fd3-4fd0-90fd-f191f1401b95"
      },
      "outputs": [],
      "source": [
        "print(win_rate_first)\n",
        "print(win_rate_second)\n",
        "\n",
        "print(tie_rate_first)\n",
        "print(tie_rate_second)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
